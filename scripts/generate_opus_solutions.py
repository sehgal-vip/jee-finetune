#!/usr/bin/env python3
"""
Generate step-by-step LaTeX solutions via Claude Opus for JEE training data.

Reads opus_queue.jsonl (generated by format_data.py), generates solutions,
reconciles against ground truth, and outputs:
  - data/opus_solutions.jsonl   (reconciled=true entries)
  - data/opus_failures.jsonl    (reconciliation failures)

Usage:
    python scripts/generate_opus_solutions.py               # full run
    python scripts/generate_opus_solutions.py --max 50      # test run (50 examples)
    python scripts/generate_opus_solutions.py --source gpqa  # filter by source
    python scripts/generate_opus_solutions.py --subject Physics
    python scripts/generate_opus_solutions.py --retry-failed # retry failures
    python scripts/generate_opus_solutions.py --report       # show stats only

Requires: ANTHROPIC_API_KEY environment variable.
"""

import argparse
import asyncio
import json
import os
import re
import sys
import time
from pathlib import Path

import anthropic
from tqdm import tqdm

DATA_DIR = Path(__file__).resolve().parent.parent / "data"
QUEUE_PATH = DATA_DIR / "opus_queue.jsonl"
SOLUTIONS_PATH = DATA_DIR / "opus_solutions.jsonl"
FAILURES_PATH = DATA_DIR / "opus_failures.jsonl"
PROGRESS_PATH = DATA_DIR / "opus_progress.jsonl"

MODEL = "claude-sonnet-4-5-20250929"
MAX_TOKENS = 4096
MAX_CONCURRENT = 10
RETRY_ATTEMPTS = 2
RETRY_DELAY = 5  # seconds
SAVE_EVERY = 100  # periodic save interval

SYSTEM_PROMPT = (
    "You are an expert IIT JEE tutor. Solve problems step-by-step using LaTeX\n"
    "notation. Show all working clearly. End every solution with the final answer\n"
    "inside \\boxed{}.\n\n"
    "For MCQ questions, explicitly evaluate each option and state why the correct\n"
    "option is correct. End with \\boxed{X} where X is the option letter.\n\n"
    "For numerical questions, show all calculation steps and end with \\boxed{value}\n"
    "including units where applicable."
)


# ---------------------------------------------------------------------------
# Answer extraction and normalization
# ---------------------------------------------------------------------------
def extract_boxed(text: str) -> str | None:
    """Extract content of \\boxed{...}, handling nested braces."""
    pattern = re.compile(r"\\boxed\{")
    m = pattern.search(text)
    if not m:
        return None
    start = m.end()
    depth = 1
    i = start
    while i < len(text) and depth > 0:
        if text[i] == "{":
            depth += 1
        elif text[i] == "}":
            depth -= 1
        i += 1
    return text[start:i - 1] if depth == 0 else text[start:]


def normalize_answer(answer: str) -> str:
    """Normalize an answer for comparison."""
    answer = answer.strip()
    # Remove common wrappers
    answer = re.sub(r"^\$(.+)\$$", r"\1", answer)
    # Strip all \text{...} wrappers recursively
    for _ in range(5):
        prev = answer
        answer = re.sub(r"\\text\{([^}]*)\}", r"\1", answer)
        if answer == prev:
            break
    # Normalize LaTeX
    answer = re.sub(r"\\[dt]frac", r"\\frac", answer)
    # Strip LaTeX formatting commands
    answer = re.sub(r"\\(mathrm|mathbf|mathit|textbf|textit|ce)\{([^}]*)\}", r"\2", answer)
    answer = re.sub(r"\\(rightarrow|to|longrightarrow)", "->", answer)
    # Remove subscript/superscript markers for plain comparison
    answer = answer.replace("_", "").replace("^", "")
    answer = answer.replace("{", "").replace("}", "")
    # Strip whitespace and extra formatting
    answer = re.sub(r"\s+", " ", answer).strip()
    return answer


def strip_to_plain(text: str) -> str:
    """Aggressively strip LaTeX to plain text for fuzzy comparison."""
    text = normalize_answer(text)
    # Remove remaining backslash commands
    text = re.sub(r"\\[a-zA-Z]+", "", text)
    # Remove special chars
    text = re.sub(r"[\\${}^_]", "", text)
    text = re.sub(r"\s+", " ", text).strip().lower()
    return text


def extract_letter(text: str) -> str | None:
    """Extract a single letter answer (A-D)."""
    text = text.strip()
    # Direct letter
    if re.match(r"^[A-Da-d]$", text):
        return text.upper()
    # Letter in parens
    m = re.match(r"^\(?([A-Da-d])\)?$", text)
    if m:
        return m.group(1).upper()
    # "Option (X)" or "Answer: X"
    m = re.search(r"(?:option|answer)\s*(?:is\s*)?\(?([A-Da-d])\)?", text, re.IGNORECASE)
    if m:
        return m.group(1).upper()
    return None


def reconcile(opus_answer: str, ground_truth: str) -> bool:
    """Check if Opus answer matches ground truth."""
    opus_norm = normalize_answer(opus_answer)
    truth_norm = normalize_answer(ground_truth)

    # Exact match
    if opus_norm.lower() == truth_norm.lower():
        return True

    # Plain text comparison (strips all LaTeX)
    opus_plain = strip_to_plain(opus_answer)
    truth_plain = strip_to_plain(ground_truth)
    if opus_plain and truth_plain and opus_plain == truth_plain:
        return True

    # Numeric tolerance (1%)
    try:
        opus_val = float(re.sub(r"[^0-9.\-eE]", "", opus_norm))
        truth_val = float(re.sub(r"[^0-9.\-eE]", "", truth_norm))
        denom = max(abs(truth_val), 1e-10)
        if abs(opus_val - truth_val) / denom < 0.01:
            return True
    except (ValueError, TypeError):
        pass

    # MCQ letter matching
    opus_letter = extract_letter(opus_norm)
    truth_letter = extract_letter(truth_norm)
    if opus_letter and truth_letter and opus_letter == truth_letter:
        return True

    # Check if truth appears within opus answer (for complex expressions)
    if truth_norm and truth_norm in opus_norm:
        return True

    # Plain text containment
    if truth_plain and len(truth_plain) >= 3 and truth_plain in opus_plain:
        return True

    return False


# ---------------------------------------------------------------------------
# API call with retry
# ---------------------------------------------------------------------------
async def generate_solution(
    client: anthropic.AsyncAnthropic,
    entry: dict,
    semaphore: asyncio.Semaphore,
) -> dict:
    """Generate a solution for a single entry. Returns result dict."""
    question = entry["question"]
    ground_truth = entry.get("ground_truth", "")
    gt_type = entry.get("ground_truth_type", "")

    user_prompt = f"**Question:**\n{question}"
    if gt_type == "mcq":
        user_prompt += "\n\nThis is a multiple-choice question. Evaluate each option."

    async with semaphore:
        for attempt in range(RETRY_ATTEMPTS + 1):
            try:
                # On retry with hint, add the ground truth
                if attempt == RETRY_ATTEMPTS and ground_truth:
                    user_prompt_with_hint = (
                        f"{user_prompt}\n\n"
                        f"[Hint: The answer is {ground_truth}. Show the complete solution.]"
                    )
                    messages = [{"role": "user", "content": user_prompt_with_hint}]
                else:
                    messages = [{"role": "user", "content": user_prompt}]

                response = await client.messages.create(
                    model=MODEL,
                    max_tokens=MAX_TOKENS,
                    system=SYSTEM_PROMPT,
                    messages=messages,
                )
                solution = response.content[0].text

                # Extract answer
                boxed = extract_boxed(solution)
                if boxed is None:
                    # Try "Answer:" fallback
                    m = re.search(r"\*?\*?Answer:?\*?\*?\s*(.+?)(?:\n|$)", solution)
                    if m:
                        boxed = m.group(1).strip()

                # Reconcile
                if boxed and ground_truth:
                    matched = reconcile(boxed, ground_truth)
                else:
                    matched = False

                if matched or attempt == RETRY_ATTEMPTS:
                    return {
                        **entry,
                        "solution": solution,
                        "opus_answer": boxed or "",
                        "reconciled": matched,
                        "attempts": attempt + 1,
                    }

                # If not matched and not last attempt, retry
                continue

            except anthropic.RateLimitError:
                wait = RETRY_DELAY * (2 ** attempt)
                await asyncio.sleep(wait)
            except anthropic.APIError as e:
                if attempt >= RETRY_ATTEMPTS:
                    return {
                        **entry,
                        "solution": "",
                        "opus_answer": "",
                        "reconciled": False,
                        "attempts": attempt + 1,
                        "error": str(e),
                    }
                await asyncio.sleep(RETRY_DELAY)

    # Should not reach here, but just in case
    return {
        **entry,
        "solution": "",
        "opus_answer": "",
        "reconciled": False,
        "attempts": RETRY_ATTEMPTS + 1,
        "error": "exhausted retries",
    }


# ---------------------------------------------------------------------------
# Batch processing with periodic saves
# ---------------------------------------------------------------------------
async def process_batch(
    entries: list[dict],
    client: anthropic.AsyncAnthropic,
    existing_ids: set[str],
) -> tuple[list[dict], list[dict]]:
    """Process a batch of entries. Returns (solutions, failures)."""
    # Filter already-processed
    to_process = [e for e in entries if e.get("id", "") not in existing_ids]
    print(f"  To process: {len(to_process)} (skipping {len(entries) - len(to_process)} already done)")

    if not to_process:
        return [], []

    semaphore = asyncio.Semaphore(MAX_CONCURRENT)
    solutions = []
    failures = []
    processed = 0

    # Create tasks
    tasks = {
        asyncio.ensure_future(generate_solution(client, entry, semaphore)): entry
        for entry in to_process
    }

    # Process with progress bar
    pbar = tqdm(total=len(tasks), desc="  Generating")
    for coro in asyncio.as_completed(tasks):
        result = await coro
        processed += 1

        if result.get("reconciled"):
            solutions.append(result)
        else:
            failures.append(result)

        pbar.update(1)
        pbar.set_postfix(ok=len(solutions), fail=len(failures))

        # Periodic save
        if processed % SAVE_EVERY == 0:
            _save_progress(solutions, failures)

    pbar.close()
    return solutions, failures


def _save_progress(solutions: list[dict], failures: list[dict]):
    """Save progress to disk."""
    with open(PROGRESS_PATH, "w") as f:
        for entry in solutions + failures:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")


def _load_existing() -> tuple[list[dict], list[dict], set[str]]:
    """Load existing solutions and failures."""
    solutions = []
    failures = []
    ids = set()

    for path in [SOLUTIONS_PATH, FAILURES_PATH, PROGRESS_PATH]:
        if path.exists():
            with open(path) as f:
                for line in f:
                    if line.strip():
                        entry = json.loads(line)
                        eid = entry.get("id", "")
                        if eid and eid not in ids:
                            ids.add(eid)
                            if entry.get("reconciled"):
                                solutions.append(entry)
                            else:
                                failures.append(entry)

    return solutions, failures, ids


# ---------------------------------------------------------------------------
# Report mode
# ---------------------------------------------------------------------------
def show_report():
    """Show stats from existing results."""
    solutions, failures, ids = _load_existing()
    total = len(solutions) + len(failures)

    if total == 0:
        print("No results found.")
        return

    print(f"\nTotal processed: {total}")
    print(f"  Reconciled (solutions): {len(solutions)} ({len(solutions)/total:.1%})")
    print(f"  Failed:                 {len(failures)} ({len(failures)/total:.1%})")

    # By source
    from collections import Counter
    sol_sources = Counter(e.get("source", "?") for e in solutions)
    fail_sources = Counter(e.get("source", "?") for e in failures)

    print(f"\nBy source:")
    all_sources = set(sol_sources) | set(fail_sources)
    for src in sorted(all_sources):
        s = sol_sources.get(src, 0)
        f = fail_sources.get(src, 0)
        t = s + f
        rate = s / t if t > 0 else 0
        print(f"  {src}: {s}/{t} reconciled ({rate:.1%})")

    # By subject
    sol_subj = Counter(e.get("subject", "?") for e in solutions)
    fail_subj = Counter(e.get("subject", "?") for e in failures)

    print(f"\nBy subject:")
    all_subj = set(sol_subj) | set(fail_subj)
    for subj in sorted(all_subj):
        s = sol_subj.get(subj, 0)
        f = fail_subj.get(subj, 0)
        t = s + f
        rate = s / t if t > 0 else 0
        print(f"  {subj}: {s}/{t} reconciled ({rate:.1%})")

    # Average attempts
    all_entries = solutions + failures
    avg_attempts = sum(e.get("attempts", 1) for e in all_entries) / len(all_entries)
    print(f"\nAverage attempts: {avg_attempts:.1f}")


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------
async def async_main():
    parser = argparse.ArgumentParser(description="Generate Opus solutions for JEE data")
    parser.add_argument("--max", type=int, default=0, help="Max examples to process (0=all)")
    parser.add_argument("--source", type=str, default="", help="Filter by source")
    parser.add_argument("--subject", type=str, default="", help="Filter by subject")
    parser.add_argument("--retry-failed", action="store_true", help="Retry from failures file")
    parser.add_argument("--report", action="store_true", help="Show stats only")
    args = parser.parse_args()

    if args.report:
        show_report()
        return

    # Check API key
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        print("Error: ANTHROPIC_API_KEY environment variable not set.")
        print("Export it: export ANTHROPIC_API_KEY='your-key-here'")
        sys.exit(1)

    print("=" * 60)
    print("Opus Solution Generation Pipeline")
    print("=" * 60)

    # Load queue
    if args.retry_failed:
        if not FAILURES_PATH.exists():
            print(f"Error: {FAILURES_PATH} not found.")
            sys.exit(1)
        entries = []
        with open(FAILURES_PATH) as f:
            for line in f:
                if line.strip():
                    entry = json.loads(line)
                    # Reset for retry
                    entry.pop("solution", None)
                    entry.pop("opus_answer", None)
                    entry.pop("reconciled", None)
                    entry.pop("attempts", None)
                    entry.pop("error", None)
                    entries.append(entry)
        print(f"Loaded {len(entries)} failed entries for retry")
    else:
        if not QUEUE_PATH.exists():
            print(f"Error: {QUEUE_PATH} not found.")
            print("Run: python scripts/format_data.py")
            sys.exit(1)
        entries = []
        with open(QUEUE_PATH) as f:
            for line in f:
                if line.strip():
                    entries.append(json.loads(line))
        print(f"Loaded {len(entries)} entries from queue")

    # Apply filters
    if args.source:
        entries = [e for e in entries if e.get("source", "") == args.source]
        print(f"Filtered by source='{args.source}': {len(entries)} entries")

    if args.subject:
        entries = [e for e in entries if e.get("subject", "").lower() == args.subject.lower()]
        print(f"Filtered by subject='{args.subject}': {len(entries)} entries")

    if args.max > 0:
        entries = entries[:args.max]
        print(f"Limited to {args.max} entries")

    if not entries:
        print("No entries to process.")
        return

    # Load existing progress
    existing_solutions, existing_failures, existing_ids = _load_existing()
    print(f"Existing progress: {len(existing_solutions)} solutions, {len(existing_failures)} failures")

    # When retrying, remove failure IDs so they get reprocessed
    if args.retry_failed:
        retry_ids = {e.get("id", "") for e in entries}
        existing_ids -= retry_ids
        existing_failures = [f for f in existing_failures if f.get("id", "") not in retry_ids]
        print(f"  Cleared {len(retry_ids)} failure IDs for retry")

    # Process
    client = anthropic.AsyncAnthropic(api_key=api_key)

    start_time = time.time()
    new_solutions, new_failures = await process_batch(entries, client, existing_ids)
    elapsed = time.time() - start_time

    # Merge with existing
    all_solutions = existing_solutions + new_solutions
    all_failures = existing_failures + new_failures

    # Write final outputs
    with open(SOLUTIONS_PATH, "w") as f:
        for entry in all_solutions:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")

    with open(FAILURES_PATH, "w") as f:
        for entry in all_failures:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")

    # Clean up progress file
    if PROGRESS_PATH.exists():
        PROGRESS_PATH.unlink()

    # Summary
    total_new = len(new_solutions) + len(new_failures)
    rate = len(new_solutions) / total_new if total_new > 0 else 0

    print(f"\n{'=' * 60}")
    print(f"Results (this run):")
    print(f"  Processed:    {total_new}")
    print(f"  Reconciled:   {len(new_solutions)} ({rate:.1%})")
    print(f"  Failed:       {len(new_failures)} ({1-rate:.1%})")
    print(f"  Time:         {elapsed:.0f}s ({elapsed/max(total_new,1):.1f}s/example)")
    print(f"\nCumulative:")
    print(f"  Solutions:    {SOLUTIONS_PATH} ({len(all_solutions)} entries)")
    print(f"  Failures:     {FAILURES_PATH} ({len(all_failures)} entries)")
    print(f"\nNext: python scripts/format_data.py --merge-opus")


def main():
    asyncio.run(async_main())


if __name__ == "__main__":
    main()

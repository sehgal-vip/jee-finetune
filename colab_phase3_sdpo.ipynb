{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: SDPO Training for IIT JEE Fine-tuning\n",
    "\n",
    "This notebook runs **Self-Distillation Preference Optimization (SDPO)** on your SFT model from Phase 2.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Download SFT model + data from HuggingFace\n",
    "2. Generate rollouts (model attempts JEE questions)\n",
    "3. Judge rollouts (rule-based + optional LLM judge)\n",
    "4. Build DPO preference pairs (chosen vs rejected)\n",
    "5. Train with DPO using TRL\n",
    "6. Upload trained model to HuggingFace\n",
    "\n",
    "**Requirements:** Colab Pro+ with A100 GPU runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers accelerate peft trl datasets bitsandbytes\n",
    "!pip install -q anthropic huggingface_hub tqdm jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Fill in your credentials below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# FILL THESE IN\n",
    "# ============================================================\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # Your HuggingFace token\n",
    "ANTHROPIC_API_KEY = \"\"  # Your Anthropic API key (optional — leave empty for rule-based judge)\n",
    "# ============================================================\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "if ANTHROPIC_API_KEY:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "\n",
    "# Model configuration\n",
    "SFT_MODEL_ID = \"vipsehgal/qwen3-8b-jee-sft\"  # Your SFT model from Phase 2\n",
    "OUTPUT_MODEL_NAME = \"qwen3-8b-jee-sdpo\"        # Name for the output model\n",
    "\n",
    "# Training configuration\n",
    "NUM_PROMPTS = 500       # Number of prompts for rollouts (reduce for faster runs)\n",
    "NUM_ROLLOUTS = 2        # Rollouts per prompt\n",
    "MAX_NEW_TOKENS = 512    # Max tokens per rollout\n",
    "DPO_EPOCHS = 2          # DPO training epochs\n",
    "DPO_BATCH_SIZE = 2      # Per-device batch size\n",
    "DPO_GRAD_ACCUM = 4      # Gradient accumulation steps (effective batch = 8)\n",
    "DPO_LR = 5e-6           # Learning rate\n",
    "DPO_BETA = 0.1          # DPO beta (higher = more conservative)\n",
    "LORA_R = 16             # LoRA rank\n",
    "LORA_ALPHA = 32         # LoRA alpha\n",
    "\n",
    "print(\"Configuration set!\")\n",
    "print(f\"  Model: {SFT_MODEL_ID}\")\n",
    "print(f\"  Prompts: {NUM_PROMPTS}, Rollouts/prompt: {NUM_ROLLOUTS}\")\n",
    "print(f\"  Judge mode: {'LLM (Claude Opus)' if ANTHROPIC_API_KEY else 'Rule-based only'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Model & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download, hf_hub_download, login\n",
    "import json\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# Download the SFT model (this takes a few minutes for 16GB)\n",
    "print(\"Downloading SFT model...\")\n",
    "snapshot_download(\n",
    "    SFT_MODEL_ID,\n",
    "    local_dir=\"./sft-model\",\n",
    "    ignore_patterns=[\"sdpo_data/*\"],  # Skip data files in first download\n",
    ")\n",
    "print(\"Model downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Download SDPO data files\n",
    "os.makedirs(\"./sdpo_data\", exist_ok=True)\n",
    "\n",
    "for fname in [\"rl_prompts.jsonl\", \"eval_prompts.jsonl\", \"judge_config.json\", \"train.jsonl\"]:\n",
    "    hf_hub_download(\n",
    "        SFT_MODEL_ID,\n",
    "        filename=f\"sdpo_data/{fname}\",\n",
    "        local_dir=\".\",\n",
    "    )\n",
    "    print(f\"Downloaded {fname}\")\n",
    "\n",
    "# Load prompts\n",
    "rl_prompts = []\n",
    "with open(\"./sdpo_data/rl_prompts.jsonl\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            rl_prompts.append(json.loads(line))\n",
    "\n",
    "eval_prompts = []\n",
    "with open(\"./sdpo_data/eval_prompts.jsonl\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            eval_prompts.append(json.loads(line))\n",
    "\n",
    "# Load training data (has gold solutions for DPO chosen responses)\n",
    "train_data = []\n",
    "with open(\"./sdpo_data/train.jsonl\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            train_data.append(json.loads(line))\n",
    "\n",
    "# Build a lookup from question -> gold solution\n",
    "gold_solutions = {}\n",
    "for item in train_data:\n",
    "    msgs = item[\"messages\"]\n",
    "    question = msgs[1][\"content\"]  # user message\n",
    "    solution = msgs[2][\"content\"]  # assistant message\n",
    "    gold_solutions[question[:200]] = solution  # key by first 200 chars\n",
    "\n",
    "print(f\"\\nLoaded {len(rl_prompts)} RL prompts, {len(eval_prompts)} eval prompts\")\n",
    "print(f\"Gold solutions available: {len(gold_solutions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Rollouts\n",
    "\n",
    "The model generates solution attempts for JEE questions. We'll judge these to create preference pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Load model in 4-bit for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"Loading model in 4-bit...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./sft-model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./sft-model\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Model loaded! Memory: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SYSTEM_MSG = (\n",
    "    \"You are an expert IIT JEE tutor. Solve problems step-by-step \"\n",
    "    \"using LaTeX notation. Show all work clearly and arrive at the final answer.\"\n",
    ")\n",
    "\n",
    "# Select a random subset of prompts\n",
    "random.seed(42)\n",
    "selected_prompts = random.sample(rl_prompts, min(NUM_PROMPTS, len(rl_prompts)))\n",
    "print(f\"Generating rollouts for {len(selected_prompts)} prompts x {NUM_ROLLOUTS} each...\")\n",
    "print(f\"Total generations: {len(selected_prompts) * NUM_ROLLOUTS}\")\n",
    "\n",
    "all_rollouts = []\n",
    "\n",
    "for prompt_data in tqdm(selected_prompts, desc=\"Generating rollouts\"):\n",
    "    question = prompt_data[\"prompt\"]\n",
    "    ground_truth = prompt_data[\"ground_truth\"]\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    for _ in range(NUM_ROLLOUTS):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        all_rollouts.append({\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"model_output\": generated,\n",
    "            \"subject\": prompt_data.get(\"subject\", \"Unknown\"),\n",
    "            \"source\": prompt_data.get(\"source\", \"unknown\"),\n",
    "        })\n",
    "\n",
    "print(f\"\\nGenerated {len(all_rollouts)} rollouts\")\n",
    "\n",
    "# Save rollouts to disk (checkpoint)\n",
    "with open(\"rollouts.jsonl\", \"w\") as f:\n",
    "    for r in all_rollouts:\n",
    "        f.write(json.dumps(r) + \"\\n\")\n",
    "print(\"Saved rollouts to rollouts.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory before training\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "print(f\"GPU memory freed: {torch.cuda.memory_allocated() / 1e9:.1f} GB used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Judge Rollouts\n",
    "\n",
    "Check each rollout for correctness. Optionally call Claude Opus for rich feedback on wrong answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "NUMERICAL_TOLERANCE = 0.01\n",
    "\n",
    "def extract_answer(response):\n",
    "    \"\"\"Extract the final answer from a model's response.\"\"\"\n",
    "    # \\boxed{...}\n",
    "    match = re.search(r'\\\\boxed\\{([^}]+)\\}', response)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    # **Answer:** ...\n",
    "    match = re.search(r'\\*\\*Answer:\\*\\*\\s*(.+?)(?:\\n|$)', response)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    # Answer: ...\n",
    "    match = re.search(r'(?:^|\\n)\\s*Answer:\\s*(.+?)(?:\\n|$)', response)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    # The answer is ...\n",
    "    match = re.search(r'[Tt]he\\s+answer\\s+is\\s+(.+?)(?:\\.|$)', response)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    # Last line with option letter\n",
    "    for line in reversed(response.strip().split('\\n')):\n",
    "        match = re.search(r'\\(([A-D])\\)', line)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def normalize_answer(answer):\n",
    "    \"\"\"Normalize an answer for comparison.\"\"\"\n",
    "    answer = str(answer).strip().lower()\n",
    "    answer = re.sub(r'^\\((.+)\\)$', r'\\1', answer)\n",
    "    answer = re.sub(r'^\\$(.+)\\$$', r'\\1', answer)\n",
    "    answer = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', answer)\n",
    "    answer = re.sub(r'[\\\\{}\\s]', '', answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def check_answer(generated, ground_truth):\n",
    "    \"\"\"Check if generated answer matches ground truth. Returns (is_correct, detail).\"\"\"\n",
    "    gen = normalize_answer(generated)\n",
    "    gt = normalize_answer(ground_truth)\n",
    "\n",
    "    if not gen:\n",
    "        return False, \"Could not extract answer\"\n",
    "    if not gt:\n",
    "        return False, \"No ground truth\"\n",
    "\n",
    "    # Exact match\n",
    "    if gen == gt:\n",
    "        return True, \"Exact match\"\n",
    "\n",
    "    # Containment\n",
    "    if gt in gen or gen in gt:\n",
    "        return True, \"Partial match\"\n",
    "\n",
    "    # Numerical comparison\n",
    "    try:\n",
    "        gen_num = float(re.sub(r'[^0-9.\\-e]', '', gen))\n",
    "        gt_num = float(re.sub(r'[^0-9.\\-e]', '', gt))\n",
    "        if gt_num != 0:\n",
    "            rel_error = abs(gen_num - gt_num) / abs(gt_num)\n",
    "            if rel_error < NUMERICAL_TOLERANCE:\n",
    "                return True, f\"Numerical match (error: {rel_error:.4f})\"\n",
    "        elif abs(gen_num - gt_num) < 1e-6:\n",
    "            return True, \"Numerical match (near zero)\"\n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        pass\n",
    "\n",
    "    # Multi-answer MCQ\n",
    "    gen_letters = set(re.findall(r'[a-d]', gen))\n",
    "    gt_letters = set(re.findall(r'[a-d]', gt))\n",
    "    if gen_letters and gt_letters:\n",
    "        if gen_letters == gt_letters:\n",
    "            return True, \"MCQ match\"\n",
    "\n",
    "    return False, f\"No match: '{generated}' vs '{ground_truth}'\"\n",
    "\n",
    "print(\"Judge functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge all rollouts\n",
    "judged_rollouts = []\n",
    "correct_count = 0\n",
    "no_answer_count = 0\n",
    "\n",
    "for rollout in tqdm(all_rollouts, desc=\"Judging rollouts\"):\n",
    "    answer = extract_answer(rollout[\"model_output\"])\n",
    "    is_correct, detail = check_answer(answer, rollout[\"ground_truth\"])\n",
    "\n",
    "    judged_rollouts.append({\n",
    "        **rollout,\n",
    "        \"answer_extracted\": answer,\n",
    "        \"is_correct\": is_correct,\n",
    "        \"detail\": detail,\n",
    "    })\n",
    "\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    if not answer:\n",
    "        no_answer_count += 1\n",
    "\n",
    "total = len(judged_rollouts)\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total rollouts: {total}\")\n",
    "print(f\"  Correct: {correct_count} ({correct_count/total*100:.1f}%)\")\n",
    "print(f\"  Incorrect: {total - correct_count} ({(total-correct_count)/total*100:.1f}%)\")\n",
    "print(f\"  No answer extracted: {no_answer_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Get rich feedback from Claude Opus for incorrect rollouts\n",
    "# This costs API credits but improves DPO training quality.\n",
    "# Skip this cell if you don't have an Anthropic API key.\n",
    "\n",
    "if ANTHROPIC_API_KEY:\n",
    "    import anthropic\n",
    "    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "    # Test the API key first\n",
    "    try:\n",
    "        test = client.messages.create(\n",
    "            model=\"claude-sonnet-4-5-20250929\",  # Use Sonnet for cost efficiency\n",
    "            max_tokens=50,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'ok'\"}],\n",
    "        )\n",
    "        print(f\"API key works! Response: {test.content[0].text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"API key error: {e}\")\n",
    "        print(\"Continuing without LLM judge.\")\n",
    "        ANTHROPIC_API_KEY = \"\"\n",
    "        client = None\n",
    "\n",
    "    if client:\n",
    "        # Get feedback for incorrect rollouts (sample to control cost)\n",
    "        incorrect = [r for r in judged_rollouts if not r[\"is_correct\"]]\n",
    "        sample_size = min(200, len(incorrect))  # Cap at 200 API calls\n",
    "        sampled = random.sample(incorrect, sample_size)\n",
    "\n",
    "        print(f\"\\nGetting LLM feedback for {sample_size} incorrect rollouts...\")\n",
    "        feedback_map = {}  # question_key -> feedback\n",
    "\n",
    "        for r in tqdm(sampled, desc=\"LLM Judge\"):\n",
    "            try:\n",
    "                resp = client.messages.create(\n",
    "                    model=\"claude-sonnet-4-5-20250929\",\n",
    "                    max_tokens=1024,\n",
    "                    system=(\n",
    "                        \"You are an expert IIT JEE examiner. Provide a correct, \"\n",
    "                        \"step-by-step solution to the question. Use LaTeX notation.\"\n",
    "                    ),\n",
    "                    messages=[{\"role\": \"user\", \"content\": (\n",
    "                        f\"Question: {r['question']}\\n\"\n",
    "                        f\"Correct answer: {r['ground_truth']}\\n\\n\"\n",
    "                        f\"Provide a clear step-by-step solution arriving at the correct answer.\"\n",
    "                    )}],\n",
    "                )\n",
    "                feedback_map[r[\"question\"][:200]] = resp.content[0].text\n",
    "            except Exception as e:\n",
    "                print(f\"API error: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Got {len(feedback_map)} LLM feedback responses\")\n",
    "else:\n",
    "    print(\"No Anthropic API key — using gold solutions from training data as chosen responses.\")\n",
    "    feedback_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build DPO Preference Dataset\n",
    "\n",
    "For each question, we create (chosen, rejected) pairs:\n",
    "- **chosen**: A correct solution (from rollout, gold training data, or LLM feedback)\n",
    "- **rejected**: An incorrect solution (from rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group rollouts by question\n",
    "question_rollouts = defaultdict(lambda: {\"correct\": [], \"incorrect\": []})\n",
    "for r in judged_rollouts:\n",
    "    key = r[\"question\"][:200]\n",
    "    if r[\"is_correct\"]:\n",
    "        question_rollouts[key][\"correct\"].append(r)\n",
    "    else:\n",
    "        question_rollouts[key][\"incorrect\"].append(r)\n",
    "\n",
    "# Build DPO pairs\n",
    "dpo_data = {\"prompt\": [], \"chosen\": [], \"rejected\": []}\n",
    "\n",
    "both_available = 0\n",
    "used_gold = 0\n",
    "used_feedback = 0\n",
    "skipped = 0\n",
    "\n",
    "for key, groups in question_rollouts.items():\n",
    "    correct_list = groups[\"correct\"]\n",
    "    incorrect_list = groups[\"incorrect\"]\n",
    "\n",
    "    if not incorrect_list:\n",
    "        # All correct — skip (model already handles this)\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    # Get the rejected response (pick one incorrect rollout)\n",
    "    rejected_response = incorrect_list[0][\"model_output\"]\n",
    "    question = incorrect_list[0][\"question\"]\n",
    "\n",
    "    # Get the chosen response (priority: correct rollout > LLM feedback > gold solution)\n",
    "    chosen_response = None\n",
    "\n",
    "    if correct_list:\n",
    "        # Best case: model got it right in another rollout\n",
    "        chosen_response = correct_list[0][\"model_output\"]\n",
    "        both_available += 1\n",
    "    elif key in feedback_map:\n",
    "        # LLM judge provided a correct solution\n",
    "        chosen_response = feedback_map[key]\n",
    "        used_feedback += 1\n",
    "    elif key in gold_solutions:\n",
    "        # Fall back to training data solution\n",
    "        chosen_response = gold_solutions[key]\n",
    "        used_gold += 1\n",
    "    else:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    # Format as chat messages (just the assistant response)\n",
    "    prompt_text = f\"{SYSTEM_MSG}\\n\\n{question}\"\n",
    "    dpo_data[\"prompt\"].append(prompt_text)\n",
    "    dpo_data[\"chosen\"].append(chosen_response)\n",
    "    dpo_data[\"rejected\"].append(rejected_response)\n",
    "\n",
    "print(f\"DPO dataset built:\")\n",
    "print(f\"  Total pairs: {len(dpo_data['prompt'])}\")\n",
    "print(f\"  From correct rollouts: {both_available}\")\n",
    "print(f\"  From LLM feedback: {used_feedback}\")\n",
    "print(f\"  From gold solutions: {used_gold}\")\n",
    "print(f\"  Skipped (all correct or no chosen): {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create HuggingFace Dataset\n",
    "dpo_dataset = Dataset.from_dict(dpo_data)\n",
    "print(f\"DPO dataset: {dpo_dataset}\")\n",
    "print(f\"\\nSample:\")\n",
    "sample = dpo_dataset[0]\n",
    "print(f\"  Prompt: {sample['prompt'][:150]}...\")\n",
    "print(f\"  Chosen: {sample['chosen'][:150]}...\")\n",
    "print(f\"  Rejected: {sample['rejected'][:150]}...\")\n",
    "\n",
    "# Save checkpoint\n",
    "dpo_dataset.save_to_disk(\"./dpo_dataset\")\n",
    "print(\"\\nSaved DPO dataset to ./dpo_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DPO Training\n",
    "\n",
    "Train the model using TRL's DPOTrainer with LoRA for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "import torch\n",
    "\n",
    "# Reload model in 4-bit for training\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"Loading model for DPO training...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./sft-model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./sft-model\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(f\"Model loaded! Memory: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO training configuration\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"./dpo-output\",\n",
    "    num_train_epochs=DPO_EPOCHS,\n",
    "    per_device_train_batch_size=DPO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=DPO_GRAD_ACCUM,\n",
    "    learning_rate=DPO_LR,\n",
    "    beta=DPO_BETA,\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",  # Change to \"wandb\" if you want W&B logging\n",
    ")\n",
    "\n",
    "# Initialize DPO trainer\n",
    "# TRL's DPOTrainer automatically handles the reference model internally\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dpo_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "print(\"DPO Trainer initialized!\")\n",
    "print(f\"  Dataset size: {len(dpo_dataset)}\")\n",
    "print(f\"  Epochs: {DPO_EPOCHS}\")\n",
    "print(f\"  Effective batch size: {DPO_BATCH_SIZE * DPO_GRAD_ACCUM}\")\n",
    "print(f\"  Total steps: ~{len(dpo_dataset) * DPO_EPOCHS // (DPO_BATCH_SIZE * DPO_GRAD_ACCUM)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting DPO training...\")\n",
    "print(\"=\"*60)\n",
    "trainer.train()\n",
    "print(\"=\"*60)\n",
    "print(\"DPO training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapters\n",
    "trainer.save_model(\"./dpo-adapters\")\n",
    "tokenizer.save_pretrained(\"./dpo-adapters\")\n",
    "print(\"LoRA adapters saved to ./dpo-adapters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Merge & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import gc\n",
    "\n",
    "# Free memory\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load base model in full precision for merging\n",
    "print(\"Loading base model for merging...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./sft-model\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./sft-model\")\n",
    "\n",
    "# Load and merge LoRA\n",
    "print(\"Merging LoRA adapters...\")\n",
    "model = PeftModel.from_pretrained(base_model, \"./dpo-adapters\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "output_dir = f\"./{OUTPUT_MODEL_NAME}\"\n",
    "print(f\"Saving merged model to {output_dir}...\")\n",
    "merged_model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Merged model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test on a few eval prompts\n",
    "del base_model, model, merged_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Loading SDPO model for evaluation...\")\n",
    "eval_model = AutoModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "if eval_tokenizer.pad_token is None:\n",
    "    eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "\n",
    "# Evaluate on a sample of eval prompts\n",
    "eval_sample = random.sample(eval_prompts, min(50, len(eval_prompts)))\n",
    "correct = 0\n",
    "\n",
    "for prompt_data in tqdm(eval_sample, desc=\"Evaluating\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": prompt_data[\"prompt\"]},\n",
    "    ]\n",
    "    input_text = eval_tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = eval_tokenizer(input_text, return_tensors=\"pt\").to(eval_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = eval_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=0.1,  # Low temp for evaluation\n",
    "            do_sample=True,\n",
    "            pad_token_id=eval_tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    generated = eval_tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    answer = extract_answer(generated)\n",
    "    is_correct, _ = check_answer(answer, prompt_data[\"ground_truth\"])\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "\n",
    "print(f\"\\nEvaluation Results (SDPO model):\")\n",
    "print(f\"  Accuracy: {correct}/{len(eval_sample)} = {correct/len(eval_sample)*100:.1f}%\")\n",
    "\n",
    "del eval_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Upload to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "HF_USERNAME = SFT_MODEL_ID.split(\"/\")[0]  # e.g. \"vipsehgal\"\n",
    "REPO_ID = f\"{HF_USERNAME}/{OUTPUT_MODEL_NAME}\"\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "# Create repo if it doesn't exist\n",
    "try:\n",
    "    api.create_repo(REPO_ID, private=True, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f\"Repo creation note: {e}\")\n",
    "\n",
    "print(f\"Uploading to {REPO_ID}...\")\n",
    "api.upload_folder(\n",
    "    folder_path=output_dir,\n",
    "    repo_id=REPO_ID,\n",
    "    repo_type=\"model\",\n",
    ")\n",
    "print(f\"\\nUpload complete!\")\n",
    "print(f\"Model: https://huggingface.co/{REPO_ID}\")\n",
    "print(f\"\\nNext: Download this model on your Mac for Phase 4 (local inference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Your SDPO-trained model is now on HuggingFace. \n",
    "\n",
    "**Phase 4 (on your Mac):**\n",
    "```bash\n",
    "# Convert to MLX 4-bit for local inference\n",
    "python -m mlx_lm.convert \\\n",
    "    --hf-path vipsehgal/qwen3-8b-jee-sdpo \\\n",
    "    --q-bits 4 --q-group-size 64 \\\n",
    "    --mlx-path ./qwen3-8b-jee-sdpo-mlx-4bit\n",
    "\n",
    "# Run inference\n",
    "mlx_lm.generate \\\n",
    "    --model ./qwen3-8b-jee-sdpo-mlx-4bit \\\n",
    "    --prompt \"Solve: A block of mass 5 kg is placed on an incline...\"\n",
    "\n",
    "# Or serve as API\n",
    "mlx_lm.server --model ./qwen3-8b-jee-sdpo-mlx-4bit --port 8080\n",
    "```"
   ]
  }
 ]
}
